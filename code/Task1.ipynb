{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0\n",
      "   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0\n",
      "  36 136 127  62  54   0   0   0   1   3   4   0   0   3   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0\n",
      "   0   0   0  12  10   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15   0   0\n",
      "   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163\n",
      " 127 121 122 146 141  88 172  66   0   0   0   0   0   0   0   0   0   1\n",
      "   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
      " 235 227 224 222 224 221 223 245 173   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243\n",
      " 202   0   0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212\n",
      " 218 192 169 227 208 218 224 212 226 197 209  52   0   0   0   0   0   0\n",
      "   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220\n",
      " 245 119 167  56   0   0   0   0   0   0   0   0   0   4   0   0  55 236\n",
      " 228 230 228 240 232 213 218 223 234 217 217 209  92   0   0   0   1   4\n",
      "   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223\n",
      " 229 215 218 255  77   0   0   3   0   0   0   0   0   0   0  62 145 204\n",
      " 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0   0   0\n",
      "   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234\n",
      " 176 188 250 248 233 238 215   0   0  57 187 208 224 221 224 208 204 214\n",
      " 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0\n",
      "   3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
      " 188 154 191 210 204 209 222 228 225   0  98 233 198 210 222 229 229 234\n",
      " 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224\n",
      " 229  29  75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195\n",
      " 227 245 239 223 218 212 209 222 220 221 230  67  48 203 183 194 213 197\n",
      " 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172\n",
      " 181 205 206 115   0 122 219 193 179 171 183 196 204 210 213 207 211 210\n",
      " 200 196 194 191 195 191 198 192 176 156 167 177 210  92   0   0  74 189\n",
      " 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188\n",
      " 188 194 192 216 170   0   2   0   0   0  66 200 222 237 239 242 246 243\n",
      " 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0   0   0\n",
      "   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "60000 784\n"
     ]
    }
   ],
   "source": [
    "# Read Fashion MNIST dataset\n",
    "import numpy as numpy\n",
    "import util_mnist_reader\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "X_train1, y_train = util_mnist_reader.load_mnist('C:\\\\Users\\\\thush\\\\Downloads\\\\Introduction to Machine Learning\\\\Project 2\\\\data\\\\fashion', kind='train')\n",
    "X_test1, y_test = util_mnist_reader.load_mnist('C:\\\\Users\\\\thush\\\\Downloads\\\\Introduction to Machine Learning\\\\Project 2\\\\data\\\\fashion', kind='t10k')\n",
    "# Your code goes here . . .\n",
    "print(X_train1[0])\n",
    "print(X_train1.shape[0],X_train1.shape[1])\n",
    "\n",
    "# Your code goes here . . .\n",
    "#X_train shape is 60,000 x 784 and y_train is 60,000 x 1\n",
    "#X_test shape is 10,000 X 784 and y_train is 10,000 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1  (128,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hidden_nodes=128\n",
    "\n",
    "w1=numpy.sqrt(2/X_train1.shape[0]) * numpy.random.rand(784,hidden_nodes)\n",
    "w2=numpy.sqrt(2/X_train1.shape[0]) * numpy.random.rand(hidden_nodes,10)\n",
    "b1=numpy.zeros((hidden_nodes)) \n",
    "b2=numpy.zeros((10)) \n",
    "learning_rate=0.1          \n",
    "print(\"b1 \",b1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train=X_train1/255.0\n",
    "X_test=X_test1/255.0\n",
    "Y_train=y_train\n",
    "Y_test=y_test\n",
    "m=len(X_train) #number of rows from data\n",
    "\n",
    "#calculating one hot vector for the loss numpy.multiply to work\n",
    "label=numpy.zeros((60000,10))\n",
    "for i in range(60000):\n",
    "    label[i,y_train[i]]=1\n",
    "Y1_train=label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sigmoid function for Hidden layer activation \n",
    "def sigmoid(z):\n",
    "    return 1/(1+numpy.exp(-(z))) \n",
    "\n",
    "def relu(x):\n",
    "    return numpy.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(z):\n",
    "    e=numpy.exp(z - numpy.max(z, axis = 1, keepdims=True))\n",
    "    return e/numpy.sum(e, axis = 1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w1, w2, b1, b2):\n",
    "    z1=numpy.dot(X,w1)+b1           \n",
    "    a_hidden = relu(z1)\n",
    "    z2=numpy.dot(a_hidden,w2)+b2                \n",
    "    a_output= softmax(z2)\n",
    "    return numpy.argmax(a_output, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.3039724865431075\n",
      "Loss:  2.2719388331180848\n",
      "Loss:  2.265861216257904\n",
      "Loss:  2.263335438171631\n",
      "Loss:  2.2614169726176527\n",
      "Loss:  2.259541154768596\n",
      "Loss:  2.2575517895144914\n",
      "Loss:  2.255382820195385\n",
      "Loss:  2.252989187141536\n",
      "Loss:  2.2503282516107137\n",
      "Loss:  2.2473544561848424\n",
      "Loss:  2.244021460566111\n",
      "Loss:  2.2402945467997415\n",
      "Loss:  2.2361735922500983\n",
      "Loss:  2.2316869399640593\n",
      "Loss:  2.226855333751633\n",
      "Loss:  2.2216826954731412\n",
      "Loss:  2.2161970789402803\n",
      "Loss:  2.210470085397697\n",
      "Loss:  2.204606348600243\n",
      "Loss:  2.1987221387790066\n",
      "Loss:  2.192902178506164\n",
      "Loss:  2.187176855362378\n",
      "Loss:  2.18152807544347\n",
      "Loss:  2.175912448571444\n",
      "Loss:  2.1702769818513987\n",
      "Loss:  2.1645596245146916\n",
      "Loss:  2.1586934094505157\n",
      "Loss:  2.1526085960289767\n",
      "Loss:  2.1462282918023785\n",
      "Loss:  2.139476183421934\n",
      "Loss:  2.1322692986764062\n",
      "Loss:  2.1245193721981934\n",
      "Loss:  2.116132361271281\n",
      "Loss:  2.1070097341850436\n",
      "Loss:  2.0970461254259516\n",
      "Loss:  2.08613453008888\n",
      "Loss:  2.0741698365861074\n",
      "Loss:  2.061049197768806\n",
      "Loss:  2.0466829435019522\n",
      "Loss:  2.03099886081911\n",
      "Loss:  2.013951281363361\n",
      "Loss:  1.9955300999000878\n",
      "Loss:  1.9757665230207144\n",
      "Loss:  1.9547410650189467\n",
      "Loss:  1.9325838854319917\n",
      "Loss:  1.9094677663611386\n",
      "Loss:  1.8855984242057573\n",
      "Loss:  1.8611988797020564\n",
      "Loss:  1.8364880705303803\n",
      "Loss:  1.8116617651698337\n",
      "Loss:  1.7868810118125555\n",
      "Loss:  1.7622666779917464\n",
      "Loss:  1.7378999340182415\n",
      "Loss:  1.7138311552687835\n",
      "Loss:  1.6900923256393425\n",
      "Loss:  1.6667091222836874\n",
      "Loss:  1.6437099308518774\n",
      "Loss:  1.6211310262063174\n",
      "Loss:  1.5990225994975968\n",
      "Loss:  1.5774469394270056\n",
      "Loss:  1.5564722248000962\n",
      "Loss:  1.536165799149153\n",
      "Loss:  1.516582083921456\n",
      "Loss:  1.4977566455831262\n",
      "Loss:  1.4797049576305146\n",
      "Loss:  1.4624214161721583\n",
      "Loss:  1.4458830966916083\n",
      "Loss:  1.4300545514411487\n",
      "Loss:  1.4148947207994844\n",
      "Loss:  1.4003578117681623\n",
      "Loss:  1.386397950549214\n",
      "Loss:  1.3729698175821894\n",
      "Loss:  1.3600295131054239\n",
      "Loss:  1.3475367647064058\n",
      "Loss:  1.3354546739950293\n",
      "Loss:  1.323750032882716\n",
      "Loss:  1.3123932017163145\n",
      "Loss:  1.301357859736096\n",
      "Loss:  1.2906211985956606\n",
      "Loss:  1.280163243698926\n",
      "Loss:  1.2699665495746248\n",
      "Loss:  1.2600159139290672\n",
      "Loss:  1.2502979327082933\n",
      "Loss:  1.2408007969244597\n",
      "Loss:  1.2315140999445846\n",
      "Loss:  1.2224284136019916\n",
      "Loss:  1.2135353828621884\n",
      "Loss:  1.2048279058481486\n",
      "Loss:  1.1962994193959022\n",
      "Loss:  1.187943751391149\n",
      "Loss:  1.179755486634006\n",
      "Loss:  1.1717297410249754\n",
      "Loss:  1.1638617952083912\n",
      "Loss:  1.1561471832585324\n",
      "Loss:  1.1485816468508194\n",
      "Loss:  1.1411613949997164\n",
      "Loss:  1.1338825361983096\n",
      "Loss:  1.1267416767520315\n",
      "Loss:  1.1197353465488868\n",
      "Loss:  1.112860288210886\n",
      "Loss:  1.1061132661932382\n",
      "Loss:  1.099491047589147\n",
      "Loss:  1.092990658421854\n",
      "Loss:  1.0866093010040594\n",
      "Loss:  1.0803440720351407\n",
      "Loss:  1.0741923913520204\n",
      "Loss:  1.0681517291419862\n",
      "Loss:  1.0622196923672118\n",
      "Loss:  1.0563941180722796\n",
      "Loss:  1.0506732552201488\n",
      "Loss:  1.0450556998366831\n",
      "Loss:  1.0395419231032468\n",
      "Loss:  1.0341384114893788\n",
      "Loss:  1.0288685033514433\n",
      "Loss:  1.023811843378355\n",
      "Loss:  1.0192205770728544\n",
      "Loss:  1.015950649905057\n",
      "Loss:  1.0164740005119206\n",
      "Loss:  1.0284942937048713\n",
      "Loss:  1.0581189526116104\n",
      "Loss:  1.1063025214173283\n",
      "Loss:  1.0815568701436409\n",
      "Loss:  1.0698108825383825\n",
      "Loss:  1.0595183385645637\n",
      "Loss:  1.0707895803286953\n",
      "Loss:  1.0507457049815656\n",
      "Loss:  1.050628873239716\n",
      "Loss:  1.0399340906659527\n",
      "Loss:  1.0424251823247406\n",
      "Loss:  1.0287892493942894\n",
      "Loss:  1.0309673877367334\n",
      "Loss:  1.0188428239371479\n",
      "Loss:  1.021953818842598\n",
      "Loss:  1.0092480426594257\n",
      "Loss:  1.013036884695004\n",
      "Loss:  1.000322368725607\n",
      "Loss:  1.004786131260323\n",
      "Loss:  0.9918880697689275\n",
      "Loss:  0.9969018454871414\n",
      "Loss:  0.9839008439473103\n",
      "Loss:  0.9893848599886832\n",
      "Loss:  0.9762998698080225\n",
      "Loss:  0.9821725542358635\n",
      "Loss:  0.9690456345941422\n",
      "Loss:  0.9752387318340842\n",
      "Loss:  0.96210123048492\n",
      "Loss:  0.9685614172846446\n",
      "Loss:  0.9554407371944814\n",
      "Loss:  0.962127883422007\n",
      "Loss:  0.9490374129724848\n",
      "Loss:  0.95591073955523\n",
      "Loss:  0.9428719188600406\n",
      "Loss:  0.949891942326174\n",
      "Loss:  0.9369349470093038\n",
      "Loss:  0.9440798201827086\n",
      "Loss:  0.931216837784816\n",
      "Loss:  0.9384519823946976\n",
      "Loss:  0.9256970708356013\n",
      "Loss:  0.9330007731987122\n",
      "Loss:  0.92036789284697\n",
      "Loss:  0.9277068038954357\n",
      "Loss:  0.9152163127424716\n",
      "Loss:  0.9225787309965453\n",
      "Loss:  0.9102388179377991\n",
      "Loss:  0.9176050501156613\n",
      "Loss:  0.9054242702740711\n",
      "Loss:  0.912774564662708\n",
      "Loss:  0.9007622655549584\n",
      "Loss:  0.9080798021041753\n",
      "Loss:  0.8962453584910401\n",
      "Loss:  0.903518003196171\n",
      "Loss:  0.8918653369781803\n",
      "Loss:  0.8990760707723737\n",
      "Loss:  0.8876126690744988\n",
      "Loss:  0.8947459120057067\n",
      "Loss:  0.8834818152491983\n",
      "Loss:  0.8905329091495542\n",
      "Loss:  0.8794664096271327\n",
      "Loss:  0.8864253114968076\n",
      "Loss:  0.8755653007305649\n",
      "Loss:  0.8824277677452775\n",
      "Loss:  0.8717682663779822\n",
      "Loss:  0.8785255705119185\n",
      "Loss:  0.8680671922747442\n",
      "Loss:  0.8747138567869047\n",
      "Loss:  0.8644625589088469\n",
      "Loss:  0.8710010013040294\n",
      "Loss:  0.8609495925134342\n",
      "Loss:  0.8673710273218813\n",
      "Loss:  0.8575181256366177\n",
      "Loss:  0.8638193360806062\n",
      "Loss:  0.8541650726392028\n",
      "Loss:  0.8603447728724585\n",
      "Loss:  0.8508895069403274\n",
      "Loss:  0.8569498766744393\n",
      "Loss:  0.8476876110966473\n",
      "Loss:  0.8536234083273317\n",
      "Loss:  0.844552154521251\n",
      "Loss:  0.8503661850332149\n",
      "Loss:  0.8414826554296831\n",
      "Loss:  0.847172555035777\n",
      "Loss:  0.8384757676368444\n",
      "Loss:  0.8440489412559055\n",
      "Loss:  0.835526353813075\n",
      "Loss:  0.8409810051230108\n",
      "Loss:  0.8326351393412954\n",
      "Loss:  0.8379725094768301\n",
      "Loss:  0.8297941605940803\n",
      "Loss:  0.8350142277880995\n",
      "Loss:  0.8270032843286024\n",
      "Loss:  0.8321105275079365\n",
      "Loss:  0.8242626273104792\n",
      "Loss:  0.8292603543646988\n",
      "Loss:  0.8215657865981415\n",
      "Loss:  0.8264559449786802\n",
      "Loss:  0.8189135601241196\n",
      "Loss:  0.8237005014437063\n",
      "Loss:  0.8163032279872694\n",
      "Loss:  0.820988497786178\n",
      "Loss:  0.8137339947206873\n",
      "Loss:  0.8183210869413183\n",
      "Loss:  0.8112032866998756\n",
      "Loss:  0.8156957375668558\n",
      "Loss:  0.8087138937630867\n",
      "Loss:  0.8131114504103544\n",
      "Loss:  0.8062552800912152\n",
      "Loss:  0.8105607281598093\n",
      "Loss:  0.8038315719861753\n",
      "Loss:  0.8080481042477915\n",
      "Loss:  0.8014403468910896\n",
      "Loss:  0.8055735032493759\n",
      "Loss:  0.7990782587214824\n",
      "Loss:  0.8031305393329191\n",
      "Loss:  0.7967463760937089\n",
      "Loss:  0.8007200592432732\n",
      "Loss:  0.7944462362472364\n",
      "Loss:  0.7983451538439728\n",
      "Loss:  0.7921777278728497\n",
      "Loss:  0.7960043714965502\n",
      "Loss:  0.78993475558971\n",
      "Loss:  0.7936919193781117\n",
      "Loss:  0.7877166741706424\n",
      "Loss:  0.7914060855533182\n",
      "Loss:  0.7855236926050037\n",
      "Loss:  0.7891465480830285\n",
      "Loss:  0.783353159372896\n",
      "Loss:  0.7869155022171191\n",
      "Loss:  0.7812078916864569\n",
      "Loss:  0.7847114446992518\n",
      "Loss:  0.7790873495500061\n",
      "Loss:  0.7825370244028759\n",
      "Loss:  0.7769921127689994\n",
      "Loss:  0.780391293272638\n",
      "Loss:  0.7749163461109445\n",
      "Loss:  0.7782624360749366\n",
      "Loss:  0.7728560564290914\n",
      "Loss:  0.7761506430484624\n",
      "Loss:  0.77081314622328\n",
      "Loss:  0.7740577053829922\n",
      "Loss:  0.7687861086935743\n",
      "Loss:  0.7719847598623136\n",
      "Loss:  0.766778094019362\n",
      "Loss:  0.769932615912384\n",
      "Loss:  0.7647895629815191\n",
      "Loss:  0.7679055317662477\n",
      "Loss:  0.762819352838304\n",
      "Loss:  0.7658958899044289\n",
      "Loss:  0.7608647568805358\n",
      "Loss:  0.763900132030191\n",
      "Loss:  0.7589253136569463\n",
      "Loss:  0.7619212325766686\n",
      "Loss:  0.7569993139082918\n",
      "Loss:  0.7599589369919844\n",
      "Loss:  0.7550892295940265\n",
      "Loss:  0.758013222367649\n",
      "Loss:  0.7531939686268646\n",
      "Loss:  0.7560878454105275\n",
      "Loss:  0.7513159351449762\n",
      "Loss:  0.754178572806471\n",
      "Loss:  0.7494548914189388\n",
      "Loss:  0.7522871601140648\n",
      "Loss:  0.747607247189386\n",
      "Loss:  0.7504079884863766\n",
      "Loss:  0.7457701046050944\n",
      "Loss:  0.7485428306695343\n",
      "Loss:  0.7439480028051184\n",
      "Loss:  0.746693730496054\n",
      "Loss:  0.7421413408469559\n",
      "Loss:  0.7448628966770157\n",
      "Loss:  0.7403507025060453\n",
      "Loss:  0.7430498274097062\n",
      "Loss:  0.7385728619323346\n",
      "Loss:  0.7412504639566838\n",
      "Loss:  0.736808948159802\n",
      "Loss:  0.7394654073492044\n",
      "Loss:  0.7350583875694384\n",
      "Loss:  0.7376953614206666\n",
      "Loss:  0.7333198617531305\n",
      "Loss:  0.7359398300636087\n",
      "Loss:  0.7315960912721344\n",
      "Loss:  0.7341988705718241\n",
      "Loss:  0.7298820562055787\n",
      "Loss:  0.7324613550720548\n",
      "Loss:  0.7281743146180171\n",
      "Loss:  0.7307322896579653\n",
      "Loss:  0.726471665751916\n",
      "Loss:  0.7290105230312782\n",
      "Loss:  0.724781203534384\n",
      "Loss:  0.7273034457979641\n",
      "Loss:  0.723103035291883\n",
      "Loss:  0.7256091628902994\n",
      "Loss:  0.7214370400026302\n",
      "Loss:  0.723926563578307\n",
      "Loss:  0.719781607603799\n",
      "Loss:  0.722257702609156\n",
      "Loss:  0.7181399853736169\n",
      "Loss:  0.7205992869454871\n",
      "Loss:  0.7165095102902194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7189534183157804\n",
      "Loss:  0.7148867592656477\n",
      "Loss:  0.7173105224662496\n",
      "Loss:  0.7132701891969987\n",
      "Loss:  0.7156772099569169\n",
      "Loss:  0.7116606161406855\n",
      "Loss:  0.7140505169904279\n",
      "Loss:  0.7100622282469663\n",
      "Loss:  0.7124348972785066\n",
      "Loss:  0.70847529059412\n",
      "Loss:  0.7108320391976198\n",
      "Loss:  0.7068997351132387\n",
      "Loss:  0.7092426589183255\n",
      "Loss:  0.7053365965881245\n",
      "Loss:  0.7076614368082124\n",
      "Loss:  0.7037818125579847\n",
      "Loss:  0.706093622478325\n",
      "Loss:  0.7022388776561367\n",
      "Loss:  0.704536374200519\n",
      "Loss:  0.7007033473101263\n",
      "Loss:  0.7029861234770134\n",
      "Loss:  0.6991761728309547\n",
      "Loss:  0.7014461674100382\n",
      "Loss:  0.6976616815864747\n",
      "Loss:  0.6999166969831919\n",
      "Loss:  0.6961587606528925\n",
      "Loss:  0.6984006897884139\n",
      "Loss:  0.6946639610121854\n",
      "Loss:  0.6968958077969962\n",
      "Loss:  0.6931798955390829\n",
      "Loss:  0.6953986772994701\n",
      "Loss:  0.6917070804357676\n",
      "Loss:  0.6939125282015592\n",
      "Loss:  0.6902451355240861\n",
      "Loss:  0.6924398504479873\n",
      "Loss:  0.6887944948263396\n",
      "Loss:  0.6909807597356562\n",
      "Loss:  0.6873563319001981\n",
      "Loss:  0.6895293975686138\n",
      "Loss:  0.6859224213690058\n",
      "Loss:  0.688082541499764\n",
      "Loss:  0.6844966005112008\n",
      "Loss:  0.6866446722051321\n",
      "Loss:  0.6830765755091147\n",
      "Loss:  0.6852102362287915\n",
      "Loss:  0.6816658217425643\n",
      "Loss:  0.6837885987641051\n",
      "Loss:  0.6802656798296951\n",
      "Loss:  0.6823759784459271\n",
      "Loss:  0.6788744771064665\n",
      "Loss:  0.6809689183818546\n",
      "Loss:  0.677490241239959\n",
      "Loss:  0.6795735287709542\n",
      "Loss:  0.6761196196864296\n",
      "Loss:  0.6781903683237808\n",
      "Loss:  0.6747586274089482\n",
      "Loss:  0.6768163219589409\n",
      "Loss:  0.6734007262721787\n",
      "Loss:  0.6754485303987083\n",
      "Loss:  0.6720552642578715\n",
      "Loss:  0.6740915444813856\n",
      "Loss:  0.6707184753418295\n",
      "Loss:  0.6727386154772597\n",
      "Loss:  0.669387980931602\n",
      "Loss:  0.6713925031415707\n",
      "Loss:  0.6680614381452512\n",
      "Loss:  0.6700522037525342\n",
      "Loss:  0.6667447481298024\n",
      "Loss:  0.6687224033779892\n",
      "Loss:  0.665436274782975\n",
      "Loss:  0.6674007877367034\n",
      "Loss:  0.6641350775405983\n",
      "Loss:  0.6660874313473609\n",
      "Loss:  0.6628469472357067\n",
      "Loss:  0.6647830390553556\n",
      "Loss:  0.6615629624161791\n",
      "Loss:  0.6634851643756374\n",
      "Loss:  0.6602901744778121\n",
      "Loss:  0.6622000363438728\n",
      "Loss:  0.6590266197811311\n",
      "Loss:  0.6609247005979181\n",
      "Loss:  0.6577750399238442\n",
      "Loss:  0.6596615258896732\n",
      "Loss:  0.6565326791176291\n",
      "Loss:  0.6584082477424155\n",
      "Loss:  0.655299069730093\n",
      "Loss:  0.6571611083675954\n",
      "Loss:  0.6540686343981247\n",
      "Loss:  0.6559148151828583\n",
      "Loss:  0.652840807394524\n",
      "Loss:  0.6546694352244936\n",
      "Loss:  0.6516183971333342\n",
      "Loss:  0.6534337691030533\n",
      "Loss:  0.6504041610295671\n",
      "Loss:  0.6522035297211548\n",
      "Loss:  0.6491991175048121\n",
      "Loss:  0.6509814943637127\n",
      "Loss:  0.6479992464674897\n",
      "Loss:  0.6497681294942629\n",
      "Loss:  0.6468099474318303\n",
      "Loss:  0.648563595878216\n",
      "Loss:  0.6456246054530207\n",
      "Loss:  0.647363328362709\n",
      "Loss:  0.6444463510666552\n",
      "Loss:  0.6461707607237086\n",
      "Loss:  0.6432728618193622\n",
      "Loss:  0.6449858330098227\n",
      "Loss:  0.6421076614223564\n",
      "Loss:  0.6438045254527457\n",
      "Loss:  0.6409528012715743\n",
      "Loss:  0.6426349143538418\n",
      "Loss:  0.6398031724509055\n",
      "Loss:  0.6414703010191644\n",
      "Loss:  0.6386642836342322\n",
      "Loss:  0.6403169507353181\n",
      "Loss:  0.637533001087912\n",
      "Loss:  0.639174765176457\n",
      "Loss:  0.6364078120556242\n",
      "Loss:  0.6380362150444644\n",
      "Loss:  0.6352885017416928\n",
      "Loss:  0.6369080507026926\n",
      "Loss:  0.6341815555294611\n",
      "Loss:  0.6357835488256488\n",
      "Loss:  0.6330775875125609\n",
      "Loss:  0.6346668570036955\n",
      "Loss:  0.6319788959148568\n",
      "Loss:  0.6335576293556433\n",
      "Loss:  0.6308865834418171\n",
      "Loss:  0.6324568387317725\n",
      "Loss:  0.6298070156749952\n",
      "Loss:  0.6313705226938846\n",
      "Loss:  0.62873833161889\n",
      "Loss:  0.6302946769976598\n",
      "Loss:  0.6276815997526888\n",
      "Loss:  0.6292233960943198\n",
      "Loss:  0.6266247276700265\n",
      "Loss:  0.6281509524388716\n",
      "Loss:  0.6255713069002599\n",
      "Loss:  0.6270806303403008\n",
      "Loss:  0.6245176998436345\n",
      "Loss:  0.6260124860676164\n",
      "Loss:  0.6234693129775082\n",
      "Loss:  0.6249489374215296\n",
      "Loss:  0.6224224253887322\n",
      "Loss:  0.623886370920077\n",
      "Loss:  0.6213750858487223\n",
      "Loss:  0.6228245199336252\n",
      "Loss:  0.6203342701617028\n",
      "Loss:  0.6217666847655582\n",
      "Loss:  0.6193029325735669\n",
      "Loss:  0.6207225136484804\n",
      "Loss:  0.6182774515495315\n",
      "Loss:  0.6196816587507074\n",
      "Loss:  0.6172545466494682\n",
      "Loss:  0.6186482625322681\n",
      "Loss:  0.6162437442464679\n",
      "Loss:  0.6176211169757768\n",
      "Loss:  0.6152334569788703\n",
      "Loss:  0.6165960466310759\n",
      "Loss:  0.6142310523814491\n",
      "Loss:  0.6155752686636202\n",
      "Loss:  0.6132317728626804\n",
      "Loss:  0.6145591469277648\n",
      "Loss:  0.6122326828178072\n",
      "Loss:  0.6135451989100786\n",
      "Loss:  0.611244380718547\n",
      "Loss:  0.6125421299561201\n",
      "Loss:  0.6102592255552233\n",
      "Loss:  0.6115413135488728\n",
      "Loss:  0.6092812224671365\n",
      "Loss:  0.6105480086929926\n",
      "Loss:  0.6083061499329687\n",
      "Loss:  0.6095629376365942\n",
      "Loss:  0.6073406521731002\n",
      "Loss:  0.6085853512792149\n",
      "Loss:  0.6063845927081343\n",
      "Loss:  0.6076137313957596\n",
      "Loss:  0.605430966373092\n",
      "Loss:  0.6066405522343694\n",
      "Loss:  0.6044713883497335\n",
      "Loss:  0.6056599174479992\n",
      "Loss:  0.6035137717014374\n",
      "Loss:  0.6046869369430649\n",
      "Loss:  0.6025630003691749\n",
      "Loss:  0.603719774107824\n",
      "Loss:  0.6016168577521929\n",
      "Loss:  0.602761079480035\n",
      "Loss:  0.6006810424242637\n",
      "Loss:  0.6018178415795462\n",
      "Loss:  0.5997602068580519\n",
      "Loss:  0.6008907965768243\n",
      "Loss:  0.5988546214784377\n",
      "Loss:  0.5999729048080202\n",
      "Loss:  0.5979581172614777\n",
      "Loss:  0.599065225199554\n",
      "Loss:  0.5970694368225135\n",
      "Loss:  0.598163976669794\n",
      "Loss:  0.5961860822028957\n",
      "Loss:  0.5972665258692346\n",
      "Loss:  0.595309385944474\n",
      "Loss:  0.5963789409569155\n",
      "Loss:  0.594443741327419\n",
      "Loss:  0.5954949422478901\n",
      "Loss:  0.5935853911485619\n",
      "Loss:  0.5946244759598338\n",
      "Loss:  0.5927419665562248\n",
      "Loss:  0.5937645754100516\n",
      "Loss:  0.5919052281313578\n",
      "Loss:  0.5929080034969345\n",
      "Loss:  0.5910734069528361\n",
      "Loss:  0.5920541446010277\n",
      "Loss:  0.5902421576729264\n",
      "Loss:  0.5911970748083886\n",
      "Loss:  0.5894160225395795\n",
      "Loss:  0.590346193258563\n",
      "Loss:  0.5885979479215577\n",
      "Loss:  0.5894960868911807\n",
      "Loss:  0.5877868279835475\n",
      "Loss:  0.58864750565254\n",
      "Loss:  0.5869823258864397\n",
      "Loss:  0.5877985900726355\n",
      "Loss:  0.5861868869601243\n",
      "Loss:  0.5869511690621398\n",
      "Loss:  0.5853988719005117\n",
      "Loss:  0.586104093810321\n",
      "Loss:  0.5846233113401699\n",
      "Loss:  0.5852603215284825\n",
      "Loss:  0.5838643810221739\n",
      "Loss:  0.5844227030223176\n",
      "Loss:  0.583121192028248\n",
      "Loss:  0.5835808108544949\n",
      "Loss:  0.5823898946450177\n",
      "Loss:  0.5827358087161671\n",
      "Loss:  0.58168107298231\n",
      "Loss:  0.5818906875236145\n",
      "Loss:  0.581000957137368\n",
      "Loss:  0.5810477160144101\n",
      "Loss:  0.5803528391967487\n",
      "Loss:  0.5802015651538248\n",
      "Loss:  0.5797316221544139\n",
      "Loss:  0.5793437619888009\n",
      "Loss:  0.5791495257738142\n",
      "Loss:  0.5784798884413687\n",
      "Loss:  0.578617362124179\n",
      "Loss:  0.5776076856760951\n",
      "Loss:  0.5781347930760288\n",
      "Loss:  0.5767262509640269\n",
      "Loss:  0.577708448419317\n",
      "Loss:  0.5758323682766625\n",
      "Loss:  0.5773399244017614\n",
      "Loss:  0.574922561949857\n",
      "Loss:  0.577030049137679\n",
      "Loss:  0.5740007415886504\n",
      "Loss:  0.576781747760284\n",
      "Loss:  0.5730785127016199\n",
      "Loss:  0.5766007053317239\n",
      "Loss:  0.5721677668248499\n",
      "Loss:  0.5764831570140551\n",
      "Loss:  0.5712950281858505\n",
      "Loss:  0.5764313578547606\n",
      "Loss:  0.5704909190889553\n",
      "Loss:  0.5764401595173116\n",
      "Loss:  0.569806163545383\n",
      "Loss:  0.5765136950877745\n",
      "Loss:  0.5693066173326775\n",
      "Loss:  0.5766522897025411\n",
      "Loss:  0.5690785545699036\n",
      "Loss:  0.5768527862043988\n",
      "Loss:  0.569193453302844\n",
      "Loss:  0.5771118772936158\n",
      "Loss:  0.5697112465782959\n",
      "Loss:  0.5773913144926696\n",
      "Loss:  0.5706421568058377\n",
      "Loss:  0.5776354514869736\n",
      "Loss:  0.5719126086376036\n",
      "Loss:  0.577765624150138\n",
      "Loss:  0.5733402532334575\n",
      "Loss:  0.5776889203722725\n",
      "Loss:  0.5746794316835406\n",
      "Loss:  0.5773326622013828\n",
      "Loss:  0.5756783510373958\n",
      "Loss:  0.5766712838284894\n",
      "Loss:  0.5761645484461813\n",
      "Loss:  0.5757105303827439\n",
      "Loss:  0.5760787273480926\n",
      "Loss:  0.5745072703889319\n",
      "Loss:  0.5754813776720106\n",
      "Loss:  0.5731399396483634\n",
      "Loss:  0.5744815275094658\n",
      "Loss:  0.5716800748914296\n",
      "Loss:  0.5732186746831216\n",
      "Loss:  0.5701930229720116\n",
      "Loss:  0.5718057484952737\n",
      "Loss:  0.568721991625019\n",
      "Loss:  0.5703258368477818\n",
      "Loss:  0.5673021819120236\n",
      "Loss:  0.5688424818272555\n",
      "Loss:  0.5659528744590155\n",
      "Loss:  0.5673963514129522\n",
      "Loss:  0.5646844205312709\n",
      "Loss:  0.5660091302456413\n",
      "Loss:  0.5635006139077523\n",
      "Loss:  0.5646937562964747\n",
      "Loss:  0.5624013319723673\n",
      "Loss:  0.5634542218257572\n",
      "Loss:  0.5613839531280709\n",
      "Loss:  0.5622849660536882\n",
      "Loss:  0.5604406649131212\n",
      "Loss:  0.561184378617862\n",
      "Loss:  0.5595665192321844\n",
      "Loss:  0.560144186393378\n",
      "Loss:  0.5587563093048695\n",
      "Loss:  0.5591590374405982\n",
      "Loss:  0.5580046325470553\n",
      "Loss:  0.5582280548441874\n",
      "Loss:  0.5573040865786746\n",
      "Loss:  0.5573397564197384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5566468023622967\n",
      "Loss:  0.5564852493652651\n",
      "Loss:  0.5560318454181988\n",
      "Loss:  0.5556629806089288\n",
      "Loss:  0.5554520466241213\n",
      "Loss:  0.5548681433386972\n",
      "Loss:  0.5549015717851771\n",
      "Loss:  0.5540956648594116\n",
      "Loss:  0.5543747939859622\n",
      "Loss:  0.5533401645899104\n",
      "Loss:  0.5538681382866879\n",
      "Loss:  0.5526005172550368\n",
      "Loss:  0.5533822787968388\n",
      "Loss:  0.5518755714047253\n",
      "Loss:  0.5529162853834843\n",
      "Loss:  0.5511588857990317\n",
      "Loss:  0.5524619678517149\n",
      "Loss:  0.5504507708178269\n",
      "Loss:  0.552020067073363\n",
      "Loss:  0.5497499159664235\n",
      "Loss:  0.5515909944349644\n",
      "Loss:  0.5490573594096829\n",
      "Loss:  0.5511701082958768\n",
      "Loss:  0.5483737381811487\n",
      "Loss:  0.5507611009283689\n",
      "Loss:  0.5477004507737506\n",
      "Loss:  0.5503617590947966\n",
      "Loss:  0.5470362622118137\n",
      "Loss:  0.5499712771883085\n",
      "Loss:  0.5463841372954705\n",
      "Loss:  0.5495878189131963\n",
      "Loss:  0.5457425110349894\n",
      "Loss:  0.5492156018965496\n",
      "Loss:  0.5451113223792518\n",
      "Loss:  0.5488472157616229\n",
      "Loss:  0.5444879416920438\n",
      "Loss:  0.5484790075096706\n",
      "Loss:  0.5438784768865962\n",
      "Loss:  0.5481176259679396\n",
      "Loss:  0.543280848361668\n",
      "Loss:  0.5477626834061039\n",
      "Loss:  0.5426973636138726\n",
      "Loss:  0.5474136595559776\n",
      "Loss:  0.5421250488484819\n",
      "Loss:  0.5470669797584558\n",
      "Loss:  0.541565858534124\n",
      "Loss:  0.54672303953188\n",
      "Loss:  0.5410201188415957\n",
      "Loss:  0.5463767063413775\n",
      "Loss:  0.5404881851735914\n",
      "Loss:  0.5460320579374012\n",
      "Loss:  0.5399690972245225\n",
      "Loss:  0.5456812768590875\n",
      "Loss:  0.5394649861335284\n",
      "Loss:  0.5453302518417203\n",
      "Loss:  0.5389802096410907\n",
      "Loss:  0.5449856915114963\n",
      "Loss:  0.538511171029236\n",
      "Loss:  0.5446363514400038\n",
      "Loss:  0.5380562749958618\n",
      "Loss:  0.5442834662286109\n",
      "Loss:  0.5376160383018298\n",
      "Loss:  0.543922486346587\n",
      "Loss:  0.5371919680046129\n",
      "Loss:  0.5435583584421857\n",
      "Loss:  0.536781448865771\n",
      "Loss:  0.5431890279174363\n",
      "Loss:  0.5363860916721424\n",
      "Loss:  0.5428124079840421\n",
      "Loss:  0.5360002208288883\n",
      "Loss:  0.5424236609293832\n",
      "Loss:  0.5356257701854097\n",
      "Loss:  0.5420234745594599\n",
      "Loss:  0.5352601948451754\n",
      "Loss:  0.5416118629642989\n",
      "Loss:  0.5349025861021185\n",
      "Loss:  0.5411863769674351\n",
      "Loss:  0.5345467272858697\n",
      "Loss:  0.5407473185180669\n",
      "Loss:  0.534195731969827\n",
      "Loss:  0.5403017026445958\n",
      "Loss:  0.5338461157250873\n",
      "Loss:  0.5398445167251036\n",
      "Loss:  0.5334982510740711\n",
      "Loss:  0.539383747714213\n",
      "Loss:  0.5331444715595344\n",
      "Loss:  0.5389035345048624\n",
      "Loss:  0.5327822658523411\n",
      "Loss:  0.5384114236123484\n",
      "Loss:  0.5324130461587737\n",
      "Loss:  0.5379102800856187\n",
      "Loss:  0.5320288578132422\n",
      "Loss:  0.5373930822792585\n",
      "Loss:  0.5316208200957983\n",
      "Loss:  0.5368597944387279\n",
      "Loss:  0.5311949049052967\n",
      "Loss:  0.5363087014881921\n",
      "Loss:  0.5307497358956575\n",
      "Loss:  0.5357547791706272\n",
      "Loss:  0.5302825488636694\n",
      "Loss:  0.5351939898041149\n",
      "Loss:  0.5297920667622339\n",
      "Loss:  0.534629031559289\n",
      "Loss:  0.5292779230610223\n",
      "Loss:  0.534065301393785\n",
      "Loss:  0.5287478542511334\n",
      "Loss:  0.5335127887352955\n",
      "Loss:  0.5281976798533418\n",
      "Loss:  0.5329708135633311\n",
      "Loss:  0.5276300276873094\n",
      "Loss:  0.5324402709351255\n",
      "Loss:  0.5270500218250653\n",
      "Loss:  0.5319349245701608\n",
      "Loss:  0.5264664149366322\n",
      "Loss:  0.5314638500075938\n",
      "Loss:  0.5258944656355236\n",
      "Loss:  0.5310481221054076\n",
      "Loss:  0.5253467485348178\n",
      "Loss:  0.5306990044691234\n",
      "Loss:  0.5248427647658486\n",
      "Loss:  0.5304334793714126\n",
      "Loss:  0.5244317942767571\n",
      "Loss:  0.530288838347148\n",
      "Loss:  0.5241718002213934\n",
      "Loss:  0.5303162126599427\n",
      "Loss:  0.5241500640601082\n",
      "Loss:  0.5305548455110395\n",
      "Loss:  0.5244679815060243\n",
      "Loss:  0.5310332785889341\n",
      "Loss:  0.5252562870956177\n",
      "Loss:  0.5317872435890585\n",
      "Loss:  0.5266576801568017\n",
      "Loss:  0.5328114377387688\n",
      "Loss:  0.5287515715972809\n",
      "Loss:  0.5340289157055309\n",
      "Loss:  0.5314536231617963\n",
      "Loss:  0.535261598854173\n",
      "Loss:  0.5344530963859028\n",
      "Loss:  0.5362475484850974\n",
      "Loss:  0.5372209746656538\n",
      "Loss:  0.5367460995396981\n",
      "Loss:  0.5391897923123998\n",
      "Loss:  0.5365920539009306\n",
      "Loss:  0.5400211976837134\n",
      "Loss:  0.5357989967765487\n",
      "Loss:  0.5397140161319323\n",
      "Loss:  0.534488194972532\n",
      "Loss:  0.5384998953535398\n",
      "Loss:  0.532853421435286\n",
      "Loss:  0.5367191102401402\n",
      "Loss:  0.531067952220467\n",
      "Loss:  0.534658430283991\n",
      "Loss:  0.5292566015050633\n",
      "Loss:  0.5325324158875637\n",
      "Loss:  0.5275226267034523\n",
      "Loss:  0.5304769739023268\n",
      "Loss:  0.5259057589264095\n",
      "Loss:  0.5285545199354639\n",
      "Loss:  0.5244270623928999\n",
      "Loss:  0.5267962523791716\n",
      "Loss:  0.5230977635928937\n",
      "Loss:  0.5252090028095036\n",
      "Loss:  0.5219173358778662\n",
      "Loss:  0.523787653539005\n",
      "Loss:  0.5208876425536271\n",
      "Loss:  0.5225356562920571\n",
      "Loss:  0.5199969774633064\n",
      "Loss:  0.5214234285924977\n",
      "Loss:  0.5192306485599041\n",
      "Loss:  0.5204380523544045\n",
      "Loss:  0.5185810121596375\n",
      "Loss:  0.5195590651539954\n",
      "Loss:  0.5180328680516397\n",
      "Loss:  0.5187728274708368\n",
      "Loss:  0.5175774360328368\n",
      "Loss:  0.5180589740476266\n",
      "Loss:  0.5172044789070113\n",
      "Loss:  0.5174089801345285\n",
      "Loss:  0.5169079161578324\n",
      "Loss:  0.5168121091005082\n",
      "Loss:  0.5166776924808291\n",
      "Loss:  0.5162530594744381\n",
      "Loss:  0.5164953411279709\n",
      "Loss:  0.5157216239494832\n",
      "Loss:  0.516349264692678\n",
      "Loss:  0.5152102536962563\n",
      "Loss:  0.5162327513332265\n",
      "Loss:  0.5147093119606392\n",
      "Loss:  0.5161235120192725\n",
      "Loss:  0.5142127704927351\n",
      "Loss:  0.5160213792465762\n",
      "Loss:  0.5137174592539885\n",
      "Loss:  0.5159165640563198\n",
      "Loss:  0.513222615950224\n",
      "Loss:  0.5158037576398661\n",
      "Loss:  0.5127326035403652\n",
      "Loss:  0.5156847748882225\n",
      "Loss:  0.5122514302308199\n",
      "Loss:  0.5155598607310878\n",
      "Loss:  0.5117787783003049\n",
      "Loss:  0.5154261114430473\n",
      "Loss:  0.5113193369811392\n",
      "Loss:  0.5152835219401507\n",
      "Loss:  0.5108730835888892\n",
      "Loss:  0.5151294059381698\n",
      "Loss:  0.5104440975576159\n",
      "Loss:  0.5149726416840534\n",
      "Loss:  0.5100306237982799\n",
      "Loss:  0.5148039935533224\n",
      "Loss:  0.5096394104468217\n",
      "Loss:  0.514634156568225\n",
      "Loss:  0.5092690775494638\n",
      "Loss:  0.5144582121602252\n",
      "Loss:  0.5089191409876666\n",
      "Loss:  0.5142759472178452\n",
      "Loss:  0.5085927488801091\n",
      "Loss:  0.5140878252439481\n",
      "Loss:  0.5082863506702021\n",
      "Loss:  0.5138880586859242\n",
      "Loss:  0.5079952524278798\n",
      "Loss:  0.5136760388414887\n",
      "Loss:  0.5077260974239511\n",
      "Loss:  0.5134597326233413\n",
      "Loss:  0.507475594969116\n",
      "Loss:  0.5132399039023189\n",
      "Loss:  0.5072418678911723\n",
      "Loss:  0.5130136990220712\n",
      "Loss:  0.5070161698101112\n",
      "Loss:  0.5127681855642953\n",
      "Loss:  0.5067949377424508\n",
      "Loss:  0.5125019025894914\n",
      "Loss:  0.5065756765477348\n",
      "Loss:  0.5122164018992561\n",
      "Loss:  0.5063560729230231\n",
      "Loss:  0.5119103691451851\n",
      "Loss:  0.506130759012567\n",
      "Loss:  0.5115853483159805\n",
      "Loss:  0.5058974100752229\n",
      "Loss:  0.5112425477516054\n",
      "Loss:  0.5056546300188466\n",
      "Loss:  0.5108810522285505\n",
      "Loss:  0.5053942413670299\n",
      "Loss:  0.5105006680893096\n",
      "Loss:  0.5051207648823768\n",
      "Loss:  0.5101050694447568\n",
      "Loss:  0.5048241193717877\n",
      "Loss:  0.5096874903592327\n",
      "Loss:  0.5045007908005346\n",
      "Loss:  0.5092498673588246\n",
      "Loss:  0.5041535074312198\n",
      "Loss:  0.5088010409790192\n",
      "Loss:  0.5037832471490626\n",
      "Loss:  0.5083406400685149\n",
      "Loss:  0.5033932279999564\n",
      "Loss:  0.507879055301508\n",
      "Loss:  0.5029851908531576\n",
      "Loss:  0.5074168304585407\n",
      "Loss:  0.5025631008583178\n",
      "Loss:  0.5069622277296234\n",
      "Loss:  0.502130353811269\n",
      "Loss:  0.5065198365353817\n",
      "Loss:  0.5016974763379399\n",
      "Loss:  0.5061050056496925\n",
      "Loss:  0.5012644762954273\n",
      "Loss:  0.5057099757723941\n",
      "Loss:  0.5008363655469413\n",
      "Loss:  0.5053442040825809\n",
      "Loss:  0.5004175403018469\n",
      "Loss:  0.5050040385264285\n",
      "Loss:  0.5000202747184447\n",
      "Loss:  0.5047033327630371\n",
      "Loss:  0.4996599584399223\n",
      "Loss:  0.5044592220657005\n",
      "Loss:  0.4993501246312784\n",
      "Loss:  0.5042779605974282\n",
      "Loss:  0.4991040309496168\n",
      "Loss:  0.5041651143505774\n",
      "Loss:  0.4989479508413504\n",
      "Loss:  0.5041367990168346\n",
      "Loss:  0.49891921656053606\n",
      "Loss:  0.5042121183174033\n",
      "Loss:  0.49903953763285747\n",
      "Loss:  0.5043844295813973\n",
      "Loss:  0.4993353424954764\n",
      "Loss:  0.5046528339842336\n",
      "Loss:  0.49983336531433514\n",
      "Loss:  0.5050052404385125\n",
      "Loss:  0.5005364037289929\n",
      "Loss:  0.5054133932246303\n",
      "Loss:  0.5014366984215244\n",
      "Loss:  0.5058377547179548\n",
      "Loss:  0.502486806343136\n",
      "Loss:  0.5062323394874821\n",
      "Loss:  0.5036014740450635\n",
      "Loss:  0.5065292729572582\n",
      "Loss:  0.5046775277591429\n",
      "Loss:  0.5066722017351263\n",
      "Loss:  0.5055760825932414\n",
      "Loss:  0.5066032005985804\n",
      "Loss:  0.5061885214146745\n",
      "Loss:  0.5062980957789505\n",
      "Loss:  0.5064273266775938\n",
      "Loss:  0.5057397170422003\n",
      "Loss:  0.5062687493573742\n",
      "Loss:  0.5049750729328066\n",
      "Loss:  0.5057575728371785\n",
      "Loss:  0.5040475388445376\n",
      "Loss:  0.5049617436552157\n",
      "Loss:  0.5030222704384151\n",
      "Loss:  0.5039746993515271\n",
      "Loss:  0.5019556100732309\n",
      "Loss:  0.5028833813084911\n",
      "Loss:  0.5008900354366861\n",
      "Loss:  0.5017383011967347\n",
      "Loss:  0.49985582313137417\n",
      "Loss:  0.5005941490476373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.49886997785419657\n",
      "Loss:  0.49947882290257706\n",
      "Loss:  0.4979569189636677\n",
      "Loss:  0.4984257409084433\n",
      "Loss:  0.49712815482586803\n",
      "Loss:  0.49744638615390807\n",
      "Loss:  0.49639501481091786\n",
      "Loss:  0.49655157233770914\n",
      "Loss:  0.4957516417713353\n",
      "Loss:  0.49573756649458833\n",
      "Loss:  0.4951974297779564\n",
      "Loss:  0.49499948650736547\n",
      "Loss:  0.4947261599483396\n",
      "Loss:  0.49433229283436075\n",
      "Loss:  0.4943301648764796\n",
      "Loss:  0.4937282194736894\n",
      "Loss:  0.494005873222026\n",
      "Loss:  0.49318253220336594\n",
      "Loss:  0.4937525677826476\n",
      "Loss:  0.49268891252598607\n",
      "Loss:  0.4935500870081021\n",
      "Loss:  0.49222975927699897\n",
      "Loss:  0.4933965665258948\n",
      "Loss:  0.49180699264323957\n",
      "Loss:  0.4932827023294027\n",
      "Loss:  0.4914108192853658\n",
      "Loss:  0.49320350032101407\n",
      "Loss:  0.49103759329841734\n",
      "Loss:  0.4931450160123143\n",
      "Loss:  0.4906828933550999\n",
      "Loss:  0.49310762021250953\n",
      "Loss:  0.4903456571202415\n",
      "Loss:  0.49308077675519546\n",
      "Loss:  0.49001871807700204\n",
      "Loss:  0.49306322918699164\n",
      "Loss:  0.4897081106808413\n",
      "Loss:  0.49305062078595396\n",
      "Loss:  0.48940677729135884\n",
      "Loss:  0.4930344359487332\n",
      "Loss:  0.48911907089981915\n",
      "Loss:  0.49301621733015455\n",
      "Loss:  0.4888527847266587\n",
      "Loss:  0.4929951414248222\n",
      "Loss:  0.48860345342871214\n",
      "Loss:  0.49296593011408485\n",
      "Loss:  0.4883794749450607\n",
      "Loss:  0.4929354425601531\n",
      "Loss:  0.48817884011178464\n"
     ]
    }
   ],
   "source": [
    "cost=[]\n",
    "for epoch in range(1000):\n",
    "\n",
    "    z1=numpy.dot(X_train,w1)+b1            \n",
    "    a_hidden = relu(z1)\n",
    "\n",
    "    z2=numpy.dot(a_hidden,w2)+b2                \n",
    "    a_output= softmax(z2)\n",
    "    \n",
    "    log_likelihood = -numpy.log(a_output[range(X_train.shape[0]), Y_train])\n",
    "    loss = numpy.sum(log_likelihood)/X_train.shape[0]\n",
    "\n",
    "    print(\"Loss: \", loss)    \n",
    "    cost.append(loss)\n",
    "    \n",
    "      \n",
    "    a_output[range(X_train.shape[0]), Y_train] -= 1\n",
    "\n",
    "    a_output /= X_train.shape[0]\n",
    "  \n",
    "    delta_w2=numpy.dot(a_hidden.T, a_output)\n",
    "\n",
    "    \n",
    "\n",
    "    wd1=numpy.dot(a_output,w2.T) \n",
    "    wd1[a_hidden<=0] = 0\n",
    "    delta_w1=numpy.dot(X_train.T,wd1) \n",
    "    \n",
    "    \n",
    "    w1=w1-numpy.multiply(learning_rate,delta_w1)\n",
    "    w2=w2-numpy.multiply(learning_rate,delta_w2)\n",
    "\n",
    "    delta_b1=numpy.sum(wd1)\n",
    "    delta_b2=numpy.sum(a_output)\n",
    "\n",
    "    b1=b1-numpy.multiply(learning_rate,delta_b1)\n",
    "    b2=b2-numpy.multiply(learning_rate,delta_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predict(X_test, w1, w2, b1, b2)==y_test).mean()*100\n",
    "y_predict=(predict(X_test, w1, w2, b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cost')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cost)\n",
    "\n",
    "#plt.plot(cost_validation)\n",
    "\n",
    "plt.title(\"Cost vs #Iterations\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 128)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[823   7  31  61  12   0  48   0  18   0]\n",
      " [  2 950  11  27   8   0   1   0   1   0]\n",
      " [ 21   6 764  10 165   1  22   0  11   0]\n",
      " [ 31  15  19 856  50   1  24   0   4   0]\n",
      " [  0   2 127  35 796   0  32   0   8   0]\n",
      " [  1   0   0   1   0 865   0  66   7  60]\n",
      " [197   4 192  53 211   0 311   0  32   0]\n",
      " [  0   0   0   0   0  28   0 921   0  51]\n",
      " [  4   1  16  10   7   6  12   7 937   0]\n",
      " [  0   0   0   0   0  12   0  47   1 940]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion=confusion_matrix(y_test,y_predict)\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\dell\\anaconda3.5.2.0\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3.5.2.0\\lib\\site-packages (from sklearn) (0.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB)\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.2.3\n",
      "    Uninstalling pip-19.2.3:\n",
      "      Successfully uninstalled pip-19.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-1rc6cpyj\\\\pip.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading https://files.pythonhosted.org/packages/76/79/60050330fe57fb59f2c53d0d11673df28c20ea9315da3652477429fc4949/scikit_learn-0.21.3-cp36-cp36m-win_amd64.whl (5.9MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in c:\\users\\dell\\anaconda3.5.2.0\\lib\\site-packages (from scikit-learn) (1.17.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in c:\\users\\dell\\anaconda3.5.2.0\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Found existing installation: scikit-learn 0.19.2\n",
      "    Uninstalling scikit-learn-0.19.2:\n",
      "      Successfully uninstalled scikit-learn-0.19.2\n",
      "Successfully installed joblib-0.14.0 scikit-learn-0.21.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.63"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict(X_test, w1, w2, b1, b2)==y_test).mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
